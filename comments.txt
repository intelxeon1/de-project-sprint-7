Спасибо за комментарии. Приятно, что алгоритмы были проанализирован, и действительно в него вкрались ошибки, часть из них при переносе из ipynb в py.
Я поправил расчет городов. Также добавил константу с требованием 27 дней.

В mart_reg, а точнее в registration_df поправил фильтракцию по номеру.

С месяцами/неделями и датами до конца не понял, понятно, что из даты можно подсчитать всё, но по требованию агрегат надо построить.
Есть, конечно, подход - ставить дату окончания месяца/недели как будто самой недели - наверное, это имеется в виду. 
Лично мне привычно работать именно в семантике месяца/недели, т.к. много лет были именно такие системные справочники, хотя в общем случае у даты больше возможностей из коробки.

Для расстояния в рекомендациях также поправил на константу.

Я согласен, что замыкания и один аппликейшен на всё не слишком удачно с точки зрения надежности и перезапуска. Было мало времени :(
Однако, переиспользуется промежуточный датафрейм и тут вопрос, а если он большой и его парковка на диск будет работать долго - имеет ли это смысл при том, 
что запись в HDFS довольно медленная операция, а persist, как я понимаю, имеет смысл только в рамках одного приложения.
По-хорошему, надо тогда выносить его в отдельный таск, затем в параллель уже запускать остальные таски с отдельными джобами. 
Тогда нивелируется чтение/запись HDFS.

У меня остался вопрос в части принципа локализации данных и Spark.
Если SPARK развернут отдельно от узлов HDFS или поднимается по требованию, то как обеспечивается параллельное чтение из HDFS с executor`ов (физических узлов) SPARK`а?
Или, если есть HDFS, то применяется более классический подход (как в MPP аля Greenplum и Veritca), когда storage и compute объединены на одной ноде?
Но ведь тогда с машстабированием сложности, т.к. нужно выравнивать узлы между собой (т.е. нельзя засунуть сервера разной производительности в кластер)?